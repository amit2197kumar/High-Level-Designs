# Latency And Throughput

## Prerequisites
![](/Images/LatencyAndThroughput01.png)

**Latency And Throughput are two most important measure of the performance of a system.**

**Latency:** Latency is basically how long it takes for data to traverse a system. More specifically how long it takes data to traverse from one point in a system, to another point in the system.

Example:

1. Latency of a network request, is how long does it take for one request to go from a client to a server and then back from the server to the client?
2. Similarly, on a machine (server), we are reading a data from disk or memory. The time that it's going to take to read that data, is also going to be referred to as latency.

Some system need to have really low latency. Example : Online multiplayer game will lag if it has high latency.

Some system really don't care much about latency, as much as they care about accuracy.

**Throughput:** Throughput is how much work a machine can perform in a given period of time. More specifically how much data can be transferred from one point in a system to another point in a system in a given amount of time.

Typically, we measure throughput in gigabits/seconds or kilobits/seconds or megabits/seconds. Example : if we say a network has a throughput of 1 gigabits/seconds that literally means that this network can support one gigabit per second (a billion bits per second)

To increase the Throughput, one can simply pay more to the cloud provider. A Better way can be to have multiple server.

Note: We canâ€™t make assumptions about latency or throughput based on each other. They are not necessarily correlated things

![](/Images/LatencyAndThroughput02.png)

## **Second conversion**
- 1 = 1000 Milliseconds
- 1 = 1000000 Microseconds (10^6)
- 1 = 1000000000 Nanoseconds (10^9)
